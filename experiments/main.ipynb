{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "from results_json import ResultsJSON\n",
    "\n",
    "import mnist_dataset\n",
    "import uci_datasets\n",
    "from difflogic import LogicLayer, GroupSum, PackBitsTensor, CompiledLogicNet\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "BITS_TO_TORCH_FLOATING_POINT_TYPE = {\n",
    "    16: torch.float16,\n",
    "    32: torch.float32,\n",
    "    64: torch.float64\n",
    "}\n",
    "\n",
    "\n",
    "def load_dataset(args):\n",
    "    validation_loader = None\n",
    "    if args.dataset == 'adult':\n",
    "        train_set = uci_datasets.AdultDataset('./data-uci', split='train', download=True, with_val=False)\n",
    "        test_set = uci_datasets.AdultDataset('./data-uci', split='test', with_val=False)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=int(1e6), shuffle=False)\n",
    "    elif args.dataset == 'breast_cancer':\n",
    "        train_set = uci_datasets.BreastCancerDataset('./data-uci', split='train', download=True, with_val=False)\n",
    "        test_set = uci_datasets.BreastCancerDataset('./data-uci', split='test', with_val=False)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=int(1e6), shuffle=False)\n",
    "    elif args.dataset.startswith('monk'):\n",
    "        style = int(args.dataset[4])\n",
    "        train_set = uci_datasets.MONKsDataset('./data-uci', style, split='train', download=True, with_val=False)\n",
    "        test_set = uci_datasets.MONKsDataset('./data-uci', style, split='test', with_val=False)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=int(1e6), shuffle=False)\n",
    "    elif args.dataset in ['mnist', 'mnist20x20']:\n",
    "        train_set = mnist_dataset.MNIST('./data-mnist', train=True, download=True, remove_border=args.dataset == 'mnist20x20')\n",
    "        test_set = mnist_dataset.MNIST('./data-mnist', train=False, remove_border=args.dataset == 'mnist20x20')\n",
    "\n",
    "        train_set_size = math.ceil((1 - args.valid_set_size) * len(train_set))\n",
    "        valid_set_size = len(train_set) - train_set_size\n",
    "        train_set, validation_set = torch.utils.data.random_split(train_set, [train_set_size, valid_set_size])\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, drop_last=True, num_workers=4)\n",
    "        validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "    elif 'cifar-10' in args.dataset:\n",
    "        transform = {\n",
    "            'cifar-10-3-thresholds': lambda x: torch.cat([(x > (i + 1) / 4).float() for i in range(3)], dim=0),\n",
    "            'cifar-10-31-thresholds': lambda x: torch.cat([(x > (i + 1) / 32).float() for i in range(31)], dim=0),\n",
    "        }[args.dataset]\n",
    "        transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(transform),\n",
    "        ])\n",
    "        train_set = torchvision.datasets.CIFAR10('./data-cifar', train=True, download=True, transform=transforms)\n",
    "        test_set = torchvision.datasets.CIFAR10('./data-cifar', train=False, transform=transforms)\n",
    "\n",
    "        train_set_size = math.ceil((1 - args.valid_set_size) * len(train_set))\n",
    "        valid_set_size = len(train_set) - train_set_size\n",
    "        train_set, validation_set = torch.utils.data.random_split(train_set, [train_set_size, valid_set_size])\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, drop_last=True, num_workers=4)\n",
    "        validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f'The data set {args.dataset} is not supported!')\n",
    "\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "\n",
    "def load_n(loader, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        for x in loader:\n",
    "            yield x\n",
    "            i += 1\n",
    "            if i == n:\n",
    "                break\n",
    "\n",
    "\n",
    "def input_dim_of_dataset(dataset):\n",
    "    return {\n",
    "        'adult': 116,\n",
    "        'breast_cancer': 51,\n",
    "        'monk1': 17,\n",
    "        'monk2': 17,\n",
    "        'monk3': 17,\n",
    "        'mnist': 784,\n",
    "        'mnist20x20': 400,\n",
    "        'cifar-10-3-thresholds': 3 * 32 * 32 * 3,\n",
    "        'cifar-10-31-thresholds': 3 * 32 * 32 * 31,\n",
    "    }[dataset]\n",
    "\n",
    "\n",
    "def num_classes_of_dataset(dataset):\n",
    "    return {\n",
    "        'adult': 2,\n",
    "        'breast_cancer': 2,\n",
    "        'monk1': 2,\n",
    "        'monk2': 2,\n",
    "        'monk3': 2,\n",
    "        'mnist': 10,\n",
    "        'mnist20x20': 10,\n",
    "        'cifar-10-3-thresholds': 10,\n",
    "        'cifar-10-31-thresholds': 10,\n",
    "    }[dataset]\n",
    "\n",
    "\n",
    "def get_model(args):\n",
    "    llkw = dict(grad_factor=args.grad_factor, connections=args.connections)\n",
    "\n",
    "    in_dim = input_dim_of_dataset(args.dataset)\n",
    "    class_count = num_classes_of_dataset(args.dataset)\n",
    "\n",
    "    logic_layers = []\n",
    "\n",
    "    arch = args.architecture\n",
    "    k = args.num_neurons\n",
    "    l = args.num_layers\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    if arch == 'randomly_connected':\n",
    "        logic_layers.append(torch.nn.Flatten())\n",
    "        logic_layers.append(LogicLayer(in_dim=in_dim, out_dim=k, **llkw))\n",
    "        for _ in range(l - 1):\n",
    "            logic_layers.append(LogicLayer(in_dim=k, out_dim=k, **llkw))\n",
    "\n",
    "        model = torch.nn.Sequential(\n",
    "            *logic_layers,\n",
    "            GroupSum(class_count, args.tau)\n",
    "        )\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(arch)\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    total_num_neurons = sum(map(lambda x: x.num_neurons, logic_layers[1:-1]))\n",
    "    print(f'total_num_neurons={total_num_neurons}')\n",
    "    total_num_weights = sum(map(lambda x: x.num_weights, logic_layers[1:-1]))\n",
    "    print(f'total_num_weights={total_num_weights}')\n",
    "    if args.experiment_id is not None:\n",
    "        results.store_results({\n",
    "            'total_num_neurons': total_num_neurons,\n",
    "            'total_num_weights': total_num_weights,\n",
    "        })\n",
    "\n",
    "    model = model.to('cuda')\n",
    "\n",
    "    print(model)\n",
    "    if args.experiment_id is not None:\n",
    "        results.store_results({'model_str': str(model)})\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "\n",
    "def train(model, x, y, loss_fn, optimizer):\n",
    "    x = model(x)\n",
    "    loss = loss_fn(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def eval(model, loader, mode):\n",
    "    orig_mode = model.training\n",
    "    with torch.no_grad():\n",
    "        model.train(mode=mode)\n",
    "        res = np.mean(\n",
    "            [\n",
    "                (model(x.to('cuda').round()).argmax(-1) == y.to('cuda')).to(torch.float32).mean().item()\n",
    "                for x, y in loader\n",
    "            ]\n",
    "        )\n",
    "        model.train(mode=orig_mode)\n",
    "    return res.item()\n",
    "\n",
    "\n",
    "def packbits_eval(model, loader):\n",
    "    orig_mode = model.training\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = np.mean(\n",
    "            [\n",
    "                (model(PackBitsTensor(x.to('cuda').reshape(x.shape[0], -1).round().bool())).argmax(-1) == y.to(\n",
    "                    'cuda')).to(torch.float32).mean().item()\n",
    "                for x, y in loader\n",
    "            ]\n",
    "        )\n",
    "        model.train(mode=orig_mode)\n",
    "    return res.item()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Train logic gate network on the various datasets.')\n",
    "\n",
    "    parser.add_argument('-eid', '--experiment_id', type=int, default=None)\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, choices=[\n",
    "        'adult', 'breast_cancer',\n",
    "        'monk1', 'monk2', 'monk3',\n",
    "        'mnist', 'mnist20x20',\n",
    "        'cifar-10-3-thresholds',\n",
    "        'cifar-10-31-thresholds',\n",
    "    ], required=True, help='the dataset to use')\n",
    "    parser.add_argument('--tau', '-t', type=float, default=10, help='the softmax temperature tau')\n",
    "    parser.add_argument('--seed', '-s', type=int, default=0, help='seed (default: 0)')\n",
    "    parser.add_argument('--batch-size', '-bs', type=int, default=128, help='batch size (default: 128)')\n",
    "    parser.add_argument('--learning-rate', '-lr', type=float, default=0.01, help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--training-bit-count', '-c', type=int, default=32, help='training bit count (default: 32)')\n",
    "\n",
    "    parser.add_argument('--implementation', type=str, default='cuda', choices=['cuda', 'python'],\n",
    "                        help='`cuda` is the fast CUDA implementation and `python` is simpler but much slower '\n",
    "                        'implementation intended for helping with the understanding.')\n",
    "\n",
    "    parser.add_argument('--packbits_eval', action='store_true', help='Use the PackBitsTensor implementation for an '\n",
    "                                                                     'additional eval step.')\n",
    "    parser.add_argument('--compile_model', action='store_true', help='Compile the final model with C for CPU.')\n",
    "\n",
    "    parser.add_argument('--num-iterations', '-ni', type=int, default=100_000, help='Number of iterations (default: 100_000)')\n",
    "    parser.add_argument('--eval-freq', '-ef', type=int, default=2_000, help='Evaluation frequency (default: 2_000)')\n",
    "\n",
    "    parser.add_argument('--valid-set-size', '-vss', type=float, default=0., help='Fraction of the train set used for validation (default: 0.)')\n",
    "    parser.add_argument('--extensive-eval', action='store_true', help='Additional evaluation (incl. valid set eval).')\n",
    "\n",
    "    parser.add_argument('--connections', type=str, default='unique', choices=['random', 'unique'])\n",
    "    parser.add_argument('--architecture', '-a', type=str, default='randomly_connected')\n",
    "    parser.add_argument('--num_neurons', '-k', type=int)\n",
    "    parser.add_argument('--num_layers', '-l', type=int)\n",
    "\n",
    "    parser.add_argument('--grad-factor', type=float, default=1.)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    print(vars(args))\n",
    "\n",
    "    assert args.num_iterations % args.eval_freq == 0, (\n",
    "        f'iteration count ({args.num_iterations}) has to be divisible by evaluation frequency ({args.eval_freq})'\n",
    "    )\n",
    "\n",
    "    if args.experiment_id is not None:\n",
    "        assert 520_000 <= args.experiment_id < 530_000, args.experiment_id\n",
    "        results = ResultsJSON(eid=args.experiment_id, path='./results/')\n",
    "        results.store_args(args)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    train_loader, validation_loader, test_loader = load_dataset(args)\n",
    "    model, loss_fn, optim = get_model(args)\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    for i, (x, y) in tqdm(\n",
    "            enumerate(load_n(train_loader, args.num_iterations)),\n",
    "            desc='iteration',\n",
    "            total=args.num_iterations,\n",
    "    ):\n",
    "        x = x.to(BITS_TO_TORCH_FLOATING_POINT_TYPE[args.training_bit_count]).to('cuda')\n",
    "        y = y.to('cuda')\n",
    "\n",
    "        loss = train(model, x, y, loss_fn, optim)\n",
    "\n",
    "        if (i+1) % args.eval_freq == 0:\n",
    "            if args.extensive_eval:\n",
    "                train_accuracy_train_mode = eval(model, train_loader, mode=True)\n",
    "                valid_accuracy_eval_mode = eval(model, validation_loader, mode=False)\n",
    "                valid_accuracy_train_mode = eval(model, validation_loader, mode=True)\n",
    "            else:\n",
    "                train_accuracy_train_mode = -1\n",
    "                valid_accuracy_eval_mode = -1\n",
    "                valid_accuracy_train_mode = -1\n",
    "            train_accuracy_eval_mode = eval(model, train_loader, mode=False)\n",
    "            test_accuracy_eval_mode = eval(model, test_loader, mode=False)\n",
    "            test_accuracy_train_mode = eval(model, test_loader, mode=True)\n",
    "\n",
    "            r = {\n",
    "                'train_acc_eval_mode': train_accuracy_eval_mode,\n",
    "                'train_acc_train_mode': train_accuracy_train_mode,\n",
    "                'valid_acc_eval_mode': valid_accuracy_eval_mode,\n",
    "                'valid_acc_train_mode': valid_accuracy_train_mode,\n",
    "                'test_acc_eval_mode': test_accuracy_eval_mode,\n",
    "                'test_acc_train_mode': test_accuracy_train_mode,\n",
    "            }\n",
    "\n",
    "            if args.packbits_eval:\n",
    "                r['train_acc_eval'] = packbits_eval(model, train_loader)\n",
    "                r['valid_acc_eval'] = packbits_eval(model, train_loader)\n",
    "                r['test_acc_eval'] = packbits_eval(model, test_loader)\n",
    "\n",
    "            if args.experiment_id is not None:\n",
    "                results.store_results(r)\n",
    "            else:\n",
    "                print(r)\n",
    "\n",
    "            if valid_accuracy_eval_mode > best_acc:\n",
    "                best_acc = valid_accuracy_eval_mode\n",
    "                if args.experiment_id is not None:\n",
    "                    results.store_final_results(r)\n",
    "                else:\n",
    "                    print('IS THE BEST UNTIL NOW.')\n",
    "\n",
    "            if args.experiment_id is not None:\n",
    "                results.save()\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    if args.compile_model:\n",
    "        print('\\n' + '='*80)\n",
    "        print(' Converting the model to C code and compiling it...')\n",
    "        print('='*80)\n",
    "\n",
    "        for opt_level in range(4):\n",
    "\n",
    "            for num_bits in [\n",
    "                # 8,\n",
    "                # 16,\n",
    "                # 32,\n",
    "                64\n",
    "            ]:\n",
    "                os.makedirs('lib', exist_ok=True)\n",
    "                save_lib_path = 'lib/{:08d}_{}.so'.format(\n",
    "                    args.experiment_id if args.experiment_id is not None else 0, num_bits\n",
    "                )\n",
    "\n",
    "                compiled_model = CompiledLogicNet(\n",
    "                    model=model,\n",
    "                    num_bits=num_bits,\n",
    "                    cpu_compiler='gcc',\n",
    "                    # cpu_compiler='clang',\n",
    "                    verbose=True,\n",
    "                )\n",
    "\n",
    "                compiled_model.compile(\n",
    "                    opt_level=1 if args.num_layers * args.num_neurons < 50_000 else 0,\n",
    "                    save_lib_path=save_lib_path,\n",
    "                    verbose=True\n",
    "                )\n",
    "\n",
    "                correct, total = 0, 0\n",
    "                with torch.no_grad():\n",
    "                    for (data, labels) in torch.utils.data.DataLoader(test_loader.dataset, batch_size=int(1e6), shuffle=False):\n",
    "                        data = torch.nn.Flatten()(data).bool().numpy()\n",
    "\n",
    "                        output = compiled_model(data, verbose=True)\n",
    "\n",
    "                        correct += (output.argmax(-1) == labels).float().sum()\n",
    "                        total += output.shape[0]\n",
    "\n",
    "                acc3 = correct / total\n",
    "                print('COMPILED MODEL', num_bits, acc3)\n",
    "\n"
   ],
   "id": "d34856ab8612b436"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
